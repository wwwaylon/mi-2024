---
title     : "Exercises: "
subtitle  : "Title"
---

```{r child = "_setup.Rmd"}
```

```{block, box.title = "Exercise 1", box.body = list(fill = "white"), box.icon = "fa-star"}
What is the main problem with listwise deletion as a method for handling missing data?
```

```{block, opts.label = "clues", box.icon = "fa-lightbulb"}
Consider how data are excluded and how it might affect your sample.
```

```{block, box.title = "Solution 1", box.icon = "fa-check", solution = T}
Listwise deletion removes entire cases with missing data, which can lead to biased estimates if the data are not missing completely at random (MCAR). It also reduces sample size, potentially diminishing statistical power.
```

```{block, box.title = "Exercise 2", box.body = list(fill = "white"), box.icon = "fa-star"}
How does pairwise deletion differ from listwise deletion, and what is its potential drawback?
```

```{block, opts.label = "clues", box.icon = "fa-lightbulb"}
Think about how pairwise deletion handles missing data in correlation or covariance calculations.
```

```{block, box.title = "Solution 2", box.icon = "fa-check", solution = T}
Pairwise deletion uses available data for each pair of variables, allowing for more data points in calculations. However, it can lead to inconsistent results because different sample sizes are used for each pair of variables, potentially leading to biased correlations or covariances.
```

```{block, box.title = "Exercise 3", box.body = list(fill = "white"), box.icon = "fa-star"}
What is the issue with mean imputation in terms of the distribution of your data?
```

```{block, opts.label = "clues", box.icon = "fa-lightbulb"}
Consider what happens to the variability in your data when missing values are replaced by the mean.
```

```{block, box.title = "Solution 3", box.icon = "fa-check", solution = T}
Mean imputation replaces missing values with the mean of the observed data, which reduces variability and distorts the data distribution. This method can lead to biased parameter estimates and underestimate standard errors, ultimately affecting statistical inference.
```

```{block, box.title = "Exercise 4", box.body = list(fill = "white"), box.icon = "fa-star"}
Why is last observation carried forward (LOCF) a problematic technique for handling missing data in longitudinal studies?
```

```{block, opts.label = "clues", box.icon = "fa-lightbulb"}
Think about how LOCF handles missing values based on past observations and its potential limitations in representing future outcomes.
```

```{block, box.title = "Solution 4", box.icon = "fa-check", solution = T}
LOCF assumes that the last observed value remains valid for missing time points, which can lead to biased estimates, particularly if the data change over time. It also ignores the natural variability in the data and may misrepresent the true trajectory of outcomes.
```

```{block, box.title = "Exercise 5", box.body = list(fill = "white"), box.icon = "fa-star"}
What is the risk of bias when the single stochastic imputation technique?
```

```{block, opts.label = "clues", box.icon = "fa-lightbulb"}
Consider how the filled in values are treated in an analysis.
```

```{block, box.title = "Solution 5", box.icon = "fa-check", solution = T}
The imputed values do not capture uncertinty related to missing data. 
```





```{block, box.title = "Exercise 6", box.body = list(fill = "white"), box.icon = "fa-star"}
qqq
```

```{block, opts.label = "clues", box.icon = "fa-lightbulb"}
qqqq
```

```{block, box.title = "Solution 6", box.icon = "fa-check", solution = T}
qqqq
```

```{block, box.title = "Exercise 7", box.body = list(fill = "white"), box.icon = "fa-star"}
qqq
```

```{block, opts.label = "clues", box.icon = "fa-lightbulb"}
qqqq
```

```{block, box.title = "Solution 7", box.icon = "fa-check", solution = T}
qqqq
```

```{block, box.title = "Exercise 8", box.body = list(fill = "white"), box.icon = "fa-star"}
qqq
```

```{block, opts.label = "clues", box.icon = "fa-lightbulb"}
qqqq
```

```{block, box.title = "Solution 8", box.icon = "fa-check", solution = T}
qqqq
```

```{block, box.title = "Exercise 9", box.body = list(fill = "white"), box.icon = "fa-star"}
qqq
```

```{block, opts.label = "clues", box.icon = "fa-lightbulb"}
qqqq
```

```{block, box.title = "Solution 9", box.icon = "fa-check", solution = T}
qqqq
```

```{block, box.title = "Exercise 10", box.body = list(fill = "white"), box.icon = "fa-star"}
qqq
```

```{block, opts.label = "clues", box.icon = "fa-lightbulb"}
qqqq
```

```{block, box.title = "Solution 10", box.icon = "fa-check", solution = T}
qqqq
```



Exercise 6:
What are the potential issues with imputing missing data based solely on a univariate approach?

Hint: Think about the relationships between variables and whether they are adequately captured.

Solution 6:
Imputing missing data using a univariate approach (e.g., replacing missing values with the mean) does not account for the relationships between variables. This can lead to inaccurate imputations, especially when variables are correlated, and can fail to preserve the structure of the data.

Exercise 7:
What is the problem with using regression imputation without validating the assumptions of the regression model?

Hint: Consider how the assumptions of the regression model might affect the accuracy of imputations.

Solution 7:
Regression imputation assumes that the relationship between variables is correctly modeled. If the assumptions of linearity, homoscedasticity, or normality are violated, the imputed values can be biased, leading to inaccurate estimates and misleading conclusions.

Exercise 8:
Why might imputation methods that ignore uncertainty lead to flawed conclusions?

Hint: Think about the importance of incorporating uncertainty in the imputation process.

Solution 8:
Imputation methods that ignore uncertainty, such as single imputation, provide a single value for each missing data point, which underestimates the variability of the estimates. This can lead to biased results, as the uncertainty in the missing data is not adequately reflected in the analysis.

Exercise 9:
What is the impact of using overly simplistic missing data handling techniques in large datasets?

Hint: Consider how simplicity might affect the complexity and accuracy of the results in large datasets.

Solution 9:
Simplistic techniques, such as mean imputation or listwise deletion, can lead to significant biases and loss of information in large datasets. These methods fail to preserve the complex relationships between variables, and can lead to misleading conclusions, especially when the missingness is not random.

Exercise 10:
How does ignoring the nature of missing data (e.g., missing at random, missing not at random) affect the validity of statistical analysis?

Hint: Reflect on how different missing data mechanisms impact your analysis.

Solution 10:
Ignoring the nature of missing data can result in biased estimates and misleading conclusions. For example, if data are missing not at random (MNAR), traditional methods like listwise deletion or mean imputation may exacerbate the bias, leading to invalid inferences. Understanding the missing data mechanism is crucial for choosing appropriate methods and ensuring valid analysis.

